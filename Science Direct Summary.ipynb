{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/edenshrian/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/edenshrian/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "import re\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import heapq\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('disable-infobars')\n",
    "#chrome_options.add_argument('headless')\n",
    "chrome_options.add_argument('--disable-notifications')\n",
    "chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "chrome_options.add_argument('--disable-popup-blocking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sceince_direct_text_extractor:\n",
    "    \n",
    "    def __init__(self,key_word):\n",
    "        \n",
    "        self.key_word = key_word\n",
    "        self.urls = []\n",
    "        self.urls_to_drop = []\n",
    "        self.text_dict = {}\n",
    "    \n",
    "    def text_extractor(self):\n",
    "        \n",
    "        self.urls = sceince_direct_text_extractor.get_urls(self.key_word)\n",
    "        self.urls_to_drop , self.text_dict = sceince_direct_text_extractor.get_dict_of_abstract_text(self.urls)\n",
    "        \n",
    "        return \n",
    "                \n",
    "    def get_urls(key_word):\n",
    "\n",
    "        driver = webdriver.Chrome(executable_path= '/Users/edenshrian/Desktop/Documents/Eden Shrian/Eden/Chrome Driver/chromedriver', options=chrome_options)\n",
    "        driver.get(\"https://www.sciencedirect.com/\")\n",
    "        driver.maximize_window()\n",
    "        driver.find_element_by_xpath(\"/html/body/div[1]/div/div/div[2]/div[1]/div/div/div/div/div/form/div/div[1]/div/div[1]/div/div/input\").send_keys(key_word)\n",
    "        driver.find_element_by_xpath(\"/html/body/div[1]/div/div/div[2]/div[1]/div/div/div/div/div/form/div/div[1]/div/div[1]/div/div/input\").send_keys(Keys.RETURN)\n",
    "        time.sleep(2.0)\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2.0)\n",
    "        index_of_num_3 = 2\n",
    "        max_search_in_web = 25\n",
    "        list_for_iteration = np.delete(np.arange(1,max_search_in_web+2),[index_of_num_3])\n",
    "        urls = [driver.find_element_by_xpath(f\"/html/body/div[1]/div/div/div/div[1]/div/div/section/div/div[2]/main/div[1]/div[2]/div[2]/div/ol/li[{num}]/div/div/h2/span/a\").get_attribute(\"href\") for num in list_for_iteration]\n",
    "        driver.close()\n",
    "\n",
    "        return urls\n",
    "    \n",
    "    def get_unique_examples():\n",
    "        \n",
    "        df_urls = pd.read_csv(\"/Users/edenshrian/Desktop/Documents/Eden Shrian/Eden/Projects/Text Summary/urls.csv\")\n",
    "        unique_examples = list(set(list(df_urls['URLS'].values)))\n",
    "\n",
    "        return unique_examples\n",
    "\n",
    "    def parse_page_text(url):\n",
    "\n",
    "        driver = webdriver.Chrome(executable_path= '/Users/edenshrian/Desktop/Documents/Eden Shrian/Eden/Chrome Driver/chromedriver', options=chrome_options)\n",
    "        driver.get(url)\n",
    "        driver.maximize_window()\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        title = driver.title\n",
    "        title = title.replace(\"-\",\"\")\n",
    "        title = title.replace(\"ScienceDirect\",\"\")\n",
    "        unique_examples = sceince_direct_text_extractor.get_unique_examples()\n",
    "\n",
    "        for example in unique_examples:\n",
    "            try:\n",
    "                text = driver.find_element_by_xpath(example).text\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        driver.close()\n",
    "\n",
    "        return title,text\n",
    "\n",
    "    def get_dict_of_abstract_text(urls):\n",
    "\n",
    "        specialChars = '!#$%^&*<>/â€¢'\n",
    "        dictionary = {}\n",
    "        urls_to_drop = []\n",
    "        for url in urls:\n",
    "            try:\n",
    "                title,text = sceince_direct_text_extractor.parse_page_text(url)\n",
    "                for char in specialChars:\n",
    "                    text = text.replace(char,'')\n",
    "                dictionary[title] = text.replace(\"\\n\",\"\")\n",
    "            except:\n",
    "                urls_to_drop = url\n",
    "\n",
    "        return urls_to_drop,dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class summarize_sceince_direct_abstract:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        \n",
    "        self.key_word = 0\n",
    "        self.num_of_sentences = 0\n",
    "        self.summary = 0\n",
    "         \n",
    "    def get_summary(self,key_word,num_of_sentences):\n",
    "        \n",
    "        self.key_word = key_word\n",
    "        self.num_of_sentences = num_of_sentences\n",
    "        self.summary = summarize_sceince_direct_abstract.summarize_info_from_sceince_direct(self.key_word,\n",
    "                                                                                            self.num_of_sentences)\n",
    "        \n",
    "        return self.summary\n",
    "\n",
    "    def create_one_long_text(key_word):\n",
    "    \n",
    "        SD_text = sceince_direct_text_extractor(key_word)\n",
    "        SD_text.text_extractor()\n",
    "        long_text = \"\".join(list(SD_text.text_dict.values()))\n",
    "\n",
    "        return long_text\n",
    "\n",
    "    def tokenize_words_to_flat_list(long_text):\n",
    "\n",
    "        text_tokenize = sent_tokenize(long_text)\n",
    "        words = [word_tokenize(sent_token) for sent_token in text_tokenize] \n",
    "        flat_word_list = [item for sublist in words for item in sublist]\n",
    "\n",
    "        return text_tokenize,flat_word_list\n",
    "\n",
    "    def create_freq_dict(flat_word_list):\n",
    "\n",
    "        freq = {}\n",
    "        for word in flat_word_list:\n",
    "            if word not in stopwords and word.isalpha():\n",
    "                if word not in freq.keys():\n",
    "                    freq[word]  = 1\n",
    "                else:\n",
    "                    freq[word] +=1\n",
    "\n",
    "        return freq\n",
    "\n",
    "    def calculate_freq_in_dict(freq_dict):\n",
    "\n",
    "        max_freq = max(freq_dict.values())\n",
    "        for word in freq_dict.keys():\n",
    "            freq_dict[word] = (freq_dict[word]/max_freq)\n",
    "\n",
    "        return freq_dict\n",
    "\n",
    "    def calculate_sentence_score(text_tokenize,flat_word_list,freq_dict):\n",
    "\n",
    "        sentence_score = {}\n",
    "        for sent_token in text_tokenize:\n",
    "            for word in flat_word_list:\n",
    "                if word.lower() in freq_dict.keys():\n",
    "                    if sent_token not in sentence_score.keys():\n",
    "                        sentence_score[sent_token] = freq_dict[word]\n",
    "                    else:\n",
    "                        sentence_score[sent_token] += freq_dict[word]\n",
    "\n",
    "        return sentence_score\n",
    "\n",
    "    def get_summary_text(sentence_score,num_of_sentences):\n",
    "\n",
    "        summary_sentences = heapq.nlargest(num_of_sentences,\n",
    "                                          sentence_score,\n",
    "                                          key = sentence_score.get)\n",
    "\n",
    "        summary = ' '.join(summary_sentences)\n",
    "\n",
    "        return summary\n",
    "\n",
    "    def summarize_info_from_sceince_direct(key_word,num_of_sentences):\n",
    "\n",
    "        long_text = summarize_sceince_direct_abstract.create_one_long_text(key_word)\n",
    "        text_tokenize,flat_word_list = summarize_sceince_direct_abstract.tokenize_words_to_flat_list(long_text)\n",
    "        freq_dict = summarize_sceince_direct_abstract.create_freq_dict(flat_word_list)\n",
    "        sentence_score = summarize_sceince_direct_abstract.calculate_sentence_score(text_tokenize,flat_word_list,freq_dict)\n",
    "        summary = summarize_sceince_direct_abstract.get_summary_text(sentence_score,num_of_sentences)\n",
    "\n",
    "        return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Summarize = summarize_sceince_direct_abstract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary1 = Summarize.get_summary(\"Deep Learning for stock predictions\",3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stock market prediction has been a classical yet challenging problem, with the attention from both economists and computer scientists. With the purpose of building an effective prediction model, both linear and machine learning tools have been explored for the past couple of decades. Lately, deep learning models have been introduced as new frontiers for this topic and the rapid development is too fast to catch up.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary2 = Summarize.get_summary(\"E-government\",3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'While much has changed regarding e-government adoption and use by cities and their residents, it remains difficult to contextualize these changes and what they might mean for the development of e-government overall. Adopting e-government services is an innovative practice and there are a number of reasons why local governments might be expected to be particularly slow when it comes to innovation. Most cities have limited communications budgets and are severely restricted as to the amount of resources that could be utilized for potential upgrades.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
